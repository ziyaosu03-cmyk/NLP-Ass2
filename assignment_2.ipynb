{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment 2: Probabilistic Models and Vector Space Applications\n",
        "\n",
        "### Assignment Overview\n",
        "\n",
        "This assignment builds on fundamental text processing techniques to explore probabilistic language modeling, text classification, and the practical application of vector space models for information retrieval. You will implement a simple n-gram language model, build a complete text classification pipeline, develop a search engine, and analyze the core components of sequence models.\n",
        "\n",
        "You are required to complete five coding-related tasks. For each task, you will be working with a specified dataset or corpus. Please submit your solutions in this single Jupyter Notebook (`.ipynb`) file, clearly marking each task. Ensure your code is well-commented and your findings are explained in markdown cells where requested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Implementing a Bigram Language Model with Laplace Smoothing (20 Marks)\n",
        "\n",
        "**Objective:** To understand the fundamentals of n-gram language models, including probability calculation, smoothing, and evaluation with perplexity.\n",
        "\n",
        "**Description:** You will implement a Bigram language model from scratch. Your model will be trained on a small corpus and will use Add-One (Laplace) smoothing to handle unseen n-grams.\n",
        "\n",
        "**Your task is to:**\n",
        "\n",
        "1.  **Implement a training function `train_bigram_model(corpus)`:**\n",
        "    * The corpus will be a list of sentences, where each sentence is a list of tokens.\n",
        "    * The function should count all unigrams and bigrams in the corpus.\n",
        "    * It should return the unigram counts, bigram counts, and the vocabulary size (V).\n",
        "\n",
        "2.  **Implement a probability function `calculate_bigram_prob(prev_word, word, unigram_counts, bigram_counts, V)`:**\n",
        "    * This function should calculate the smoothed probability of a `word` given the `prev_word` using the formula for Laplace (Add-One) smoothing: $P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$.\n",
        "\n",
        "3.  **Implement a perplexity calculation function `calculate_perplexity(sentence, ...)`:**\n",
        "    * This function should take a test sentence and your trained model components as input.\n",
        "    * It should calculate the perplexity of the sentence using the formula: $PP(W) = P(w_1, w_2, ..., w_N)^{-1/N}$. Remember to handle the start of the sentence appropriately (e.g., by assuming a start token `<S>`).\n",
        "\n",
        "4.  **Train and Evaluate:**\n",
        "    * Train your model on the provided `train_corpus`.\n",
        "    * Calculate and print the perplexity of your model on the `test_sentence`.\n",
        "\n",
        "**Corpus:**\n",
        "\n",
        "```python\n",
        "# Sample corpus for training and testing\n",
        "train_corpus = [[\"<S>\", \"i\", \"am\", \"sam\", \"</S>\"], [\"<S>\", \"sam\", \"i\", \"am\", \"</S>\"], [\"<S>\", \"i\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\", \"ham\", \"</S>\"]]\n",
        "test_sentence = [\"<S>\", \"i\", \"like\", \"green\", \"ham\", \"</S>\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for Task 1 here\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Provided Corpus\n",
        "train_corpus = [[\"<S>\", \"i\", \"am\", \"sam\", \"</S>\"], [\"<S>\", \"sam\", \"i\", \"am\", \"</S>\"], [\"<S>\", \"i\", \"do\", \"not\", \"like\", \"green\", \"eggs\", \"and\", \"ham\", \"</S>\"]]\n",
        "test_sentence = [\"<S>\", \"i\", \"like\", \"green\", \"ham\", \"</S>\"]\n",
        "\n",
        "def train_bigram_model(corpus):\n",
        "    \"\"\"\n",
        "    ?? Bigram ????\n",
        "    \n",
        "    ??:\n",
        "        corpus: ????????????? token ??\n",
        "    \n",
        "    ??:\n",
        "        unigram_counts: unigram ????\n",
        "        bigram_counts: bigram ????\n",
        "        V: ?????\n",
        "    \"\"\"\n",
        "    unigram_counts = Counter()\n",
        "    bigram_counts = defaultdict(Counter)\n",
        "    \n",
        "    # ???? unigram ? bigram\n",
        "    for sentence in corpus:\n",
        "        # ?? unigram\n",
        "        for token in sentence:\n",
        "            unigram_counts[token] += 1\n",
        "        \n",
        "        # ?? bigram\n",
        "        for i in range(len(sentence) - 1):\n",
        "            prev_word = sentence[i]\n",
        "            word = sentence[i + 1]\n",
        "            bigram_counts[prev_word][word] += 1\n",
        "    \n",
        "    # ????????????? token?\n",
        "    V = len(unigram_counts)\n",
        "    \n",
        "    return unigram_counts, bigram_counts, V\n",
        "\n",
        "def calculate_bigram_prob(prev_word, word, unigram_counts, bigram_counts, V):\n",
        "    \"\"\"\n",
        "    ???? Laplace ??? bigram ??\n",
        "    \n",
        "    ??: P(w_i | w_{i-1}) = (C(w_{i-1}, w_i) + 1) / (C(w_{i-1}) + V)\n",
        "    \n",
        "    ??:\n",
        "        prev_word: ????\n",
        "        word: ???\n",
        "        unigram_counts: unigram ????\n",
        "        bigram_counts: bigram ????\n",
        "        V: ?????\n",
        "    \n",
        "    ??:\n",
        "        ???????\n",
        "    \"\"\"\n",
        "    # ?? bigram ?????????? 0\n",
        "    bigram_count = bigram_counts.get(prev_word, Counter()).get(word, 0)\n",
        "    \n",
        "    # ??????? unigram ?????????? 0\n",
        "    prev_word_count = unigram_counts.get(prev_word, 0)\n",
        "    \n",
        "    # ?? Laplace ????\n",
        "    prob = (bigram_count + 1) / (prev_word_count + V)\n",
        "    \n",
        "    return prob\n",
        "\n",
        "def calculate_perplexity(sentence, unigram_counts, bigram_counts, V):\n",
        "    \"\"\"\n",
        "    ????????\n",
        "    \n",
        "    ??: PP(W) = P(w_1, w_2, ..., w_N)^{-1/N}\n",
        "    ?? P(w_1, w_2, ..., w_N) = ? P(w_i | w_{i-1})\n",
        "    \n",
        "    ??:\n",
        "        sentence: ?????token ??\n",
        "        unigram_counts: unigram ????\n",
        "        bigram_counts: bigram ????\n",
        "        V: ?????\n",
        "    \n",
        "    ??:\n",
        "        ????\n",
        "    \"\"\"\n",
        "    # ???? bigram ?????\n",
        "    log_prob_sum = 0.0\n",
        "    \n",
        "    # ???????? bigram\n",
        "    for i in range(len(sentence) - 1):\n",
        "        prev_word = sentence[i]\n",
        "        word = sentence[i + 1]\n",
        "        \n",
        "        # ????\n",
        "        prob = calculate_bigram_prob(prev_word, word, unigram_counts, bigram_counts, V)\n",
        "        \n",
        "        # ??????????\n",
        "        log_prob_sum += np.log(prob)\n",
        "    \n",
        "    # ??????????????? </S>???????? bigram ???\n",
        "    N = len(sentence) - 1\n",
        "    \n",
        "    # ?????: PP(W) = exp(-log_prob_sum / N)\n",
        "    perplexity = np.exp(-log_prob_sum / N)\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Train the model\n",
        "unigram_counts, bigram_counts, V = train_bigram_model(train_corpus)\n",
        "\n",
        "# Calculate and print perplexity\n",
        "perplexity = calculate_perplexity(test_sentence, unigram_counts, bigram_counts, V)\n",
        "print(f\"Perplexity of the test sentence: {perplexity:.2f}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Text Classification with TF-IDF and Naive Bayes (20 Marks)\n",
        "\n",
        "**Objective:** To build a complete text classification pipeline using TF-IDF feature extraction and a Multinomial Naive Bayes classifier.\n",
        "\n",
        "**Description:** You will use `scikit-learn` to classify SMS messages as either \"spam\" or \"ham\" (not spam). This task integrates vector space representation with a classic probabilistic model.\n",
        "\n",
        "**Your task is to:**\n",
        "\n",
        "1.  Load the SMS Spam Collection dataset.\n",
        "2.  Split the dataset into an 80% training set and a 20% testing set.\n",
        "3.  Create a text processing pipeline using `sklearn.pipeline.Pipeline` that consists of two steps:\n",
        "    * `TfidfVectorizer`: To convert text messages into TF-IDF vectors. Use the default parameters.\n",
        "    * `MultinomialNB`: The Multinomial Naive Bayes classifier.\n",
        "4.  Train the pipeline on the training data.\n",
        "5.  Evaluate the trained model on the testing data. Print the following:\n",
        "    * The accuracy of the model.\n",
        "    * A full classification report (including precision, recall, and F1-score for each class) using `sklearn.metrics.classification_report`.\n",
        "6.  Use the trained pipeline to predict the class of two new messages: `\"Congratulations! You've won a $1,000 gift card. Go to http://example.com to claim now.\"` and `\"Hi mom, I'll be home for dinner tonight.\"`\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "* **SMS Spam Collection Dataset:** A public set of SMS labeled messages.\n",
        "* **Access:** Download from the UCI Machine Learning Repository: [SMS Spam Collection](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You will need the `SMSSpamCollection` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for Task 2 here\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset, e.g, as follows. But you may modify it.\n",
        "# df = pd.read_csv('path_to_your_dataset/SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Split data\n",
        "\n",
        "# Create and train the pipeline\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "# Predict on new messages\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Building a Simple Information Retrieval System (20 Marks)\n",
        "\n",
        "**Objective:** To apply TF-IDF and Cosine Similarity to build a basic document retrieval system that ranks documents based on their relevance to a query.\n",
        "\n",
        "**Description:** You will create a system that takes a text query and returns the most relevant documents from a small corpus. This is the core principle behind search engines.\n",
        "\n",
        "**Your task is to:**\n",
        "\n",
        "1.  Use the provided `document_corpus`.\n",
        "2.  Create a `TfidfVectorizer` and fit it on the corpus to learn the vocabulary and IDF weights.\n",
        "3.  Transform the corpus into a TF-IDF document-term matrix.\n",
        "4.  Write a function `rank_documents(query, vectorizer, doc_term_matrix, top_n=3)` that:\n",
        "    * Takes a `query` string, the fitted `vectorizer`, the document-term `matrix`, and an optional `top_n` integer.\n",
        "    * Transforms the input query into a TF-IDF vector using the *same* vectorizer.\n",
        "    * Calculates the cosine similarity between the query vector and all document vectors in the matrix.\n",
        "    * Returns the indices and content of the `top_n` most similar documents.\n",
        "5.  Demonstrate your system by running it with the query `\"deep learning models for vision\"` and printing the ranked results.\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "```python\n",
        "# A small corpus of document abstracts\n",
        "document_corpus = [\n",
        "    \"The field of machine learning has seen rapid growth in recent years, especially in deep learning.\",\n",
        "    \"Natural language processing allows machines to understand and respond to human text.\",\n",
        "    \"Computer vision focuses on enabling computers to see and interpret the visual world.\",\n",
        "    \"Deep learning models like convolutional neural networks are powerful for computer vision tasks.\",\n",
        "    \"Recurrent neural networks are often used for sequential data in natural language processing.\"\n",
        "    ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for Task 3 here\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "document_corpus = [\n",
        "    \"The field of machine learning has seen rapid growth in recent years, especially in deep learning.\",\n",
        "    \"Natural language processing allows machines to understand and respond to human text.\",\n",
        "    \"Computer vision focuses on enabling computers to see and interpret the visual world.\",\n",
        "    \"Deep learning models like convolutional neural networks are powerful for computer vision tasks.\",\n",
        "    \"Recurrent neural networks are often used for sequential data in natural language processing.\",\n",
        "    \"The advances in reinforcement learning have led to breakthroughs in game playing and robotics.\",\n",
        "    \"Transfer learning enables models trained on large datasets to be adapted for new tasks with limited data.\",\n",
        "    \"Unsupervised learning techniques can discover hidden patterns in data without labeled examples.\",\n",
        "    \"Optimization algorithms such as stochastic gradient descent are crucial for training neural networks.\",\n",
        "    \"Attention mechanisms have improved the performance of natural language translation and image captioning.\",\n",
        "    \"Generative adversarial networks create realistic images and are used for data augmentation.\",\n",
        "    \"Feature engineering and selection are important steps in classical machine learning pipelines.\",\n",
        "    \"Object detection is a key task in computer vision that involves locating instances within images.\",\n",
        "    \"The combination of convolutional and recurrent networks is used for video classification tasks.\",\n",
        "    \"Zero-shot learning allows models to recognize objects and concepts they have not seen during training.\",\n",
        "    \"Natural language generation is used for creating text summaries and chatbot responses.\",\n",
        "    \"Graph neural networks leverage graph structures for tasks such as social network analysis and chemistry.\",\n",
        "    \"Hyperparameter tuning can significantly improve the accuracy of deep learning models.\",\n",
        "    \"Cross-modal learning involves integrating information from multiple data sources such as text and images.\",\n",
        "    \"Evaluating model performance requires a good choice of metrics such as F1-score and RMSE.\"\n",
        "]\n",
        "\n",
        "# Create and fit the vectorizer\n",
        "\n",
        "# Transform the corpus to DTM\n",
        "\n",
        "def rank_documents(query, vectorizer, doc_term_matrix, top_n=3):\n",
        "    # Implement the ranking function\n",
        "    pass\n",
        "\n",
        "# Demonstrate with a sample query\n",
        "query = \"deep learning models for vision\"\n",
        "# ranked_docs = rank_documents(query, vectorizer, doc_term_matrix)\n",
        "# print(f\"Top {len(ranked_docs)} documents for the query: '{query}'\\n\")\n",
        "# for idx, doc in ranked_docs:\n",
        "#     print(f\"Rank {idx+1}: {doc}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4: Implementing Viterbi for HMM POS Tagging (20 Marks)\n",
        "\n",
        "**Objective:** Implement a simple Hidden Markov Model (HMM) POS tagger via the Viterbi algorithm.\n",
        "\n",
        "**Description:** Implement Viterbi decoding for a small HMM and apply it to two sentences with the ambiguous word \"book\". Then briefly discuss why HMMs work for POS tagging and a limitation of the Markov assumption.\n",
        "\n",
        "**Your task is to:**\n",
        "\n",
        "1. Define two sentences:\n",
        "    * `sentence1 = \"The book is on the table.\"`\n",
        "    * `sentence2 = \"I want to book a flight.\"`\n",
        "2. Implement Viterbi in log-space for a small tag set (e.g., `{DET, NOUN, VERB, PRT}`). Use the example initial (?), transition (A), and emission (B) probabilities in the parameters block below, or define your own consistent matrices and document them.\n",
        "3. Run your decoder on both sentences and print the predicted tag sequence and total log-probability.\n",
        "4. In a markdown cell, explain:\n",
        "    * **a)** How transition and emission probabilities lead to different tags for \"book\" in the two sentences.\n",
        "    * **b)** One sentence where the first-order Markov assumption is limiting, and why.\n",
        "\n",
        "**Parameters:** Use the matrices shown in the section ?Viterbi decoding for a simple HMM (Task 4)? below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Viterbi decoding for a simple HMM (Task 4)\n",
        "\n",
        "We illustrate HMM POS tagging with a small tag set `T = {DET, NOUN, VERB, PRT}` and vocabulary `V = {the, a, book, table, flight, is, want, to, on, i}`. The HMM comprises initial probabilities ?, tag-to-tag transitions A, and tag-to-word emissions B.\n",
        "\n",
        "Example parameters (each row sums to 1):\n",
        "\n",
        "- Initial ?:\n",
        "  - P(DET)=0.50, P(NOUN)=0.20, P(VERB)=0.20, P(PRT)=0.10\n",
        "\n",
        "- Transition A (rows: from-tag, cols: to-tag) in order [DET, NOUN, VERB, PRT]:\n",
        "\n",
        "```text\n",
        "from\\to   DET    NOUN   VERB   PRT\n",
        "DET      0.05   0.75   0.15   0.05\n",
        "NOUN     0.05   0.10   0.75   0.10\n",
        "VERB     0.10   0.35   0.40   0.15\n",
        "PRT      0.05   0.10   0.75   0.10\n",
        "```\n",
        "\n",
        "- Emission B:\n",
        "  - DET: the(0.80), a(0.20)\n",
        "  - NOUN: book(0.45), table(0.25), flight(0.20), i(0.05), on(0.05)\n",
        "  - VERB: is(0.40), want(0.35), book(0.20), to(0.03), on(0.02)\n",
        "  - PRT: to(0.70), on(0.30)\n",
        "\n",
        "Viterbi recurrence in log-space to avoid underflow:\n",
        "\n",
        "- Initialization: `V[tag, 0] = log ?[tag] + log B[tag, x0]`\n",
        "- Recurrence: `V[tag, i] = log B[tag, xi] + max_prev ( V[prev, i-1] + log A[prev->tag] )`\n",
        "- Backtrace from the best final tag.\n",
        "\n",
        "We will decode the most likely tag sequence for the two Task 4 sentences using these parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Task 4: Implement Viterbi for a simple POS HMM (skeleton)\n",
        "import math\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Define your tag set\n",
        "TAGS = [\"DET\", \"NOUN\", \"VERB\", \"PRT\"]\n",
        "\n",
        "# Define HMM parameters (fill using the given parameters)\n",
        "pi: Dict[str, float] = {\n",
        "    # \"DET\": ..., \"NOUN\": ..., \"VERB\": ..., \"PRT\": ...\n",
        "}\n",
        "\n",
        "A: Dict[str, Dict[str, float]] = {\n",
        "    # \"DET\": {\"DET\": ..., \"NOUN\": ..., \"VERB\": ..., \"PRT\": ...},\n",
        "    # \"NOUN\": {\"DET\": ..., \"NOUN\": ..., \"VERB\": ..., \"PRT\": ...},\n",
        "    # \"VERB\": {\"DET\": ..., \"NOUN\": ..., \"VERB\": ..., \"PRT\": ...},\n",
        "    # \"PRT\": {\"DET\": ..., \"NOUN\": ..., \"VERB\": ..., \"PRT\": ...},\n",
        "}\n",
        "\n",
        "B: Dict[str, Dict[str, float]] = {\n",
        "    # \"DET\": {\"the\": ..., \"a\": ...},\n",
        "    # \"NOUN\": {\"book\": ..., \"table\": ..., \"flight\": ..., \"i\": ..., \"on\": ...},\n",
        "    # \"VERB\": {\"is\": ..., \"want\": ..., \"book\": ..., \"to\": ..., \"on\": ...},\n",
        "    # \"PRT\": {\"to\": ..., \"on\": ...},\n",
        "}\n",
        "\n",
        "UNK = 1e-8\n",
        "\n",
        "# Return log-probability for emitting 'word' from 'tag'. Use UNK for unseen words.\n",
        "def emission_logprob(tag: str, word: str) -> float:\n",
        "    # TODO: implement emission in log-space\n",
        "    pass\n",
        "\n",
        "# Implement Viterbi in log-space\n",
        "# 1) initialize  2) dynamic programming with backpointers  3) termination + backtrace\n",
        "def viterbi(tokens: List[str]) -> Tuple[List[str], float]:\n",
        "    # TODO: implement Viterbi\n",
        "    pass\n",
        "\n",
        "# Prepare the two sentences (lowercased tokens)\n",
        "sentence1 = [\"the\", \"book\", \"is\", \"on\", \"the\", \"table\"]\n",
        "sentence2 = [\"i\", \"want\", \"to\", \"book\", \"a\", \"flight\"]\n",
        "\n",
        "# Run your decoder and print outputs\n",
        "# tags1, logp1 = viterbi(sentence1)\n",
        "# print(list(zip(sentence1, tags1)), \" | logP=\", round(logp1, 3))\n",
        "# tags2, logp2 = viterbi(sentence2)\n",
        "# print(list(zip(sentence2, tags2)), \" | logP=\", round(logp2, 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analysis for Task 4**\n",
        "\n",
        "* How transition and emission probabilities lead to different tags for \"book\" in the two sentences.\n",
        "\n",
        "\n",
        "* One sentence where the first-order Markov assumption is limiting, and why.\n",
        "\n",
        "Your analysis goes here    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 5: Comparing Cosine Similarity and Euclidean Distance (20 Marks)\n",
        "\n",
        "**Objective:** To empirically demonstrate the difference between angle-based (Cosine) and magnitude-based (Euclidean) similarity measures in a vector space.\n",
        "\n",
        "**Description:** The choice of similarity metric is crucial. This task highlights how document length affects each metric and why Cosine Similarity is often preferred for text-based topic similarity.\n",
        "\n",
        "**Your task is to:**\n",
        "\n",
        "1.  Define three simple documents:\n",
        "    * `doc_A = \"The cat sat on the mat.\"`\n",
        "    * `doc_B = \"The cat sat on the mat. The dog chased the cat.\"` (Longer, but on the same topic)\n",
        "    * `doc_C = \"The rocket launched into space.\"` (Different topic)\n",
        "2.  Use `sklearn.feature_extraction.text.CountVectorizer` to transform these three documents into count vectors.\n",
        "3.  Calculate the **Cosine Similarity** between all unique pairs of documents (A-B, A-C, B-C).\n",
        "4.  Calculate the **Euclidean Distance** between all unique pairs of documents.\n",
        "5.  Display your results clearly, for instance, in a Pandas DataFrame.\n",
        "6.  **In a markdown cell, analyze your results:**\n",
        "    * Explain why the Cosine Similarity between `doc_A` and `doc_B` is high, while their Euclidean Distance is relatively large.\n",
        "    * Which metric (Cosine Similarity or Euclidean Distance) do your results suggest is better for identifying documents with similar topics, regardless of their length? Justify your answer based on your calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code for Task 5 here\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "import pandas as pd\n",
        "\n",
        "# Define documents\n",
        "doc_A = \"The cat sat on the mat.\"\n",
        "doc_B = \"The cat sat on the mat. The dog chased the cat.\"\n",
        "doc_C = \"The rocket launched into space.\"\n",
        "corpus = [doc_A, doc_B, doc_C]\n",
        "\n",
        "# Vectorize the documents\n",
        "\n",
        "# Calculate similarities and distances\n",
        "\n",
        "# Display results in a DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Analysis for Task 5**\n",
        "\n",
        "* Explain why the Cosine Similarity between `doc_A` and `doc_B` is high, while their Euclidean Distance is relatively large.\n",
        "\n",
        "* Which metric (Cosine Similarity or Euclidean Distance) do your results suggest is better for identifying documents with similar topics, regardless of their length? Justify your answer based on your calculations.\n",
        "\n",
        "*Your analysis goes here.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}